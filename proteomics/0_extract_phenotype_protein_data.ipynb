{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 0: extract proteomics data\n",
    "\n",
    "This notebook extracts proteomics data via the Cohort Browser. This notebook extracts all ~1500 proteins measured in the first tranche of proteomics data released. This notebook also demonstrates how users can extract all proteomics data available from 500K participants or extract proteomics data associated with a specific phenotype or cohort.\n",
    "\n",
    "The output is a dataframe that is samples x proteins."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As-Is Software Disclaimer\n",
    "\n",
    "This notebook is delivered \"As-Is\". Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.\n",
    "\n",
    "[MIT License](https://github.com/dnanexus/UKB_RAP/blob/main/LICENSE) applies to this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterLab app details\n",
    "\n",
    "<b>Launch spec:</b>\n",
    "- App name: JupyterLab\n",
    "- Instance type: Spark cluster, 2 nodes\n",
    "- Runtime: =~ 5 mins\n",
    "- Cost ~= Â£0.47"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "|Library |License|\n",
    "|:------------- |:-------------|\n",
    "|[pandas](https://pandas.pydata.org/) |[BSD-3](https://github.com/pandas-dev/pandas/blob/main/LICENSE)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# dxpy allows python to interact with the platform storage\n",
    "# Note: This notebook is using spark since the size of the dataset we're extracting\n",
    "# (i.e. the number of fields) is too large for a single node instance.\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/alee_example/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically discover dispensed dataset ID\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")\n",
    "dispensed_dataset_id = dispensed_dataset[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project ID\n",
    "project_id = dxpy.find_one_project()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\":\").join([project_id, dispensed_dataset_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell can only be run once. Otherwise, you'll need to delete the existing data tables in order to re-run\n",
    "cmd = [\"dx\", \"extract_dataset\", dataset, \"-ddd\", \"--delimiter\", \",\"]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_csv = glob.glob(os.path.join(path, \"*.data_dictionary.csv\"))[0]\n",
    "data_dict_df = pd.read_csv(data_dict_csv)\n",
    "data_dict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = list(\n",
    "    data_dict_df.loc[data_dict_df[\"entity\"] == \"olink_instance_0\", \"name\"].values\n",
    ")\n",
    "print(len(field_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names_str = [f\"olink_instance_0.{f}\" for f in field_names]\n",
    "field_names_query = \",\".join(field_names_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export field name list to file for Table Exporter\n",
    "# Alternatively, instead of using dx extract_dataset you can use the Table exporter app\n",
    "# This list of field names can be used as input into the Table exporter app and then\n",
    "# you can ignore running the remaining cells in this notebook\n",
    "\n",
    "# file = open('field_names.txt','w')\n",
    "# for item in field_names:\n",
    "#    file.write(item+\"\\n\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dx upload field_names.txt --destination /alee_example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract from full 500K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"128m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large number of proteins that we are extracting, `dx extract_dataset` fails using a single node instance. To resolve this we create an SQL query that we later use in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    dataset,\n",
    "    \"--fields\",\n",
    "    field_names_query,\n",
    "    \"--delimiter\",\n",
    "    \",\",\n",
    "    \"--output\",\n",
    "    \"extracted_data.sql\",\n",
    "    \"--sql\",\n",
    "]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_data.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\"\n",
    "    for line in file:\n",
    "        retrieve_sql += line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf.shape)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract from cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover cohort data\n",
    "dispensed_control_id = list(\n",
    "    dxpy.find_data_objects(\n",
    "        typename=\"CohortBrowser\",\n",
    "        folder=\"/alee_example\",\n",
    "        name_mode=\"exact\",\n",
    "        name=\"ischaemic_control\",\n",
    "    )\n",
    ")[0][\"id\"]\n",
    "\n",
    "dispensed_case_id = list(\n",
    "    dxpy.find_data_objects(\n",
    "        typename=\"CohortBrowser\",\n",
    "        folder=\"/alee_example\",\n",
    "        name_mode=\"exact\",\n",
    "        name=\"ischaemic_cases\",\n",
    "    )\n",
    ")[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dataset = (\":\").join([project_id, dispensed_control_id])\n",
    "case_dataset = (\":\").join([project_id, dispensed_case_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    control_dataset,\n",
    "    \"--fields\",\n",
    "    field_names_query,\n",
    "    \"--delimiter\",\n",
    "    \",\",\n",
    "    \"--output\",\n",
    "    \"extracted_control_data.sql\",\n",
    "    \"--sql\",\n",
    "]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    case_dataset,\n",
    "    \"--fields\",\n",
    "    field_names_query,\n",
    "    \"--delimiter\",\n",
    "    \",\",\n",
    "    \"--output\",\n",
    "    \"extracted_case_data.sql\",\n",
    "    \"--sql\",\n",
    "]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_control_data.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\"\n",
    "    for line in file:\n",
    "        retrieve_sql += line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(control_df.shape)\n",
    "control_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_case_data.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\"\n",
    "    for line in file:\n",
    "        retrieve_sql += line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(case_df.shape)\n",
    "case_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Mar 10 2023, 20:16:38) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
